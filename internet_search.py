import aiohttp
import asyncio
from bs4 import BeautifulSoup
import re
from concurrent.futures import ThreadPoolExecutor
class InternetSearcher:
    def __init__(self):
        self.session = None
    
    async def search_online(self, query, max_results=3):
        """–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –ø—Ä–∏ –Ω–µ–∑–Ω–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        try:
            print(f"üîç –ë–æ—Ç –Ω–µ –∑–Ω–∞–µ—Ç –æ—Ç–≤–µ—Ç, –∏—â—É –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {query}")
            
            from googlesearch import search
            
            results = []
            # –ò—â–µ–º –≤ Google –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                search_results = await loop.run_in_executor(
                    executor,
                    lambda: list(search(query, num_results=max_results, lang="ru"))
                )

            async with aiohttp.ClientSession() as session:
                tasks = []
                for url in search_results[:max_results]:
                    tasks.append(self.fetch_page_content(session, url))
                
                pages_content = await asyncio.gather(*tasks, return_exceptions=True)
                
                for url, content in zip(search_results, pages_content):
                    if content and not isinstance(content, Exception):
                        results.append({
                            'url': url,
                            'content': self.clean_content(content)[:300]
                        })
            
            return results
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    async def fetch_page_content(self, session, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            async with session.get(url, timeout=10) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    text = soup.get_text()
                    return text
                return None
        except:
            return None
    
    def clean_content(self, text):
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç"""
        if not text:
            return ""
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def should_search_internet(self, ai_response, user_question):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–∫–∞—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
        question_lower = user_question.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å (—Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞)
        factual_keywords = [
            "–∫–æ–≥–¥–∞", "–≥–¥–µ", "–ø–æ—á–µ–º—É", "–∫–∞–∫", "—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫—Ç–æ —Ç–∞–∫–æ–π", "–∫—Ç–æ —Ç–∞–∫–∞—è",
            "–∫–∞–∫–æ–µ", "–∫–∞–∫–æ–π", "–∫–∞–∫–∞—è", "–∫–∞–∫–∏–µ",  # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ "–∫–∞–∫–æ–µ –±—ã–ª–æ –ø–µ—Ä–≤–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ"
            "–ø–µ—Ä–≤–æ–µ", "–ø–æ—Å–ª–µ–¥–Ω–µ–µ", "–ø–æ—Å–ª–µ–¥–Ω–∏–π", "–ø–µ—Ä–≤—ã–π", "–ø–µ—Ä–≤–∞—è",  # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è—Ö
            "—Å–∫–æ–ª—å–∫–æ", "–∫–∞–∫–æ–π –≥–æ–¥", "–≤ –∫–∞–∫–æ–º –≥–æ–¥—É", "–¥–∞—Ç–∞",  # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –¥–∞—Ç–∞—Ö
            "–≥–¥–µ —Ä–æ–¥–∏–ª—Å—è", "–≥–¥–µ —É–º–µ—Ä", "—Ä–æ–¥–∏–Ω–∞", "–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å",  # –î–ª—è –±–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
            "–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ", "–∫–Ω–∏–≥–∞", "—Ä–æ–º–∞–Ω", "—Ä–∞—Å—Å–∫–∞–∑", "—Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ", "–ø—å–µ—Å–∞",  # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–∞–±–æ—Ç–∞—Ö
            "–Ω–∞–ø–∏—Å–∞–ª", "—Å–æ–∑–¥–∞–ª", "–∞–≤—Ç–æ—Ä—Å—Ç–≤–æ"  # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ–± –∞–≤—Ç–æ—Ä—Å—Ç–≤–µ
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤–æ–ø—Ä–æ—Å —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º
        is_factual_question = any(keyword in question_lower for keyword in factual_keywords)
        
        # –ï—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–æ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å - –í–°–ï–ì–î–ê –∏—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        if is_factual_question:
            return True
        
        # –ò–Ω–∞—á–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ñ—Ä–∞–∑—ã –Ω–µ–∑–Ω–∞–Ω–∏—è –≤ –æ—Ç–≤–µ—Ç–µ (–¥–∞–∂–µ –¥–ª—è —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤)
        unknown_phrases = [
            "–Ω–µ –∑–Ω–∞—é", "–Ω–µ —É–≤–µ—Ä–µ–Ω", "–Ω–µ –º–æ–≥—É —Å–∫–∞–∑–∞—Ç—å", "–Ω–µ –∏–º–µ—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
            "–Ω–µ —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é –¥–∞–Ω–Ω—ã–º–∏", "–∑–∞—Ç—Ä—É–¥–Ω—è—é—Å—å –æ—Ç–≤–µ—Ç–∏—Ç—å", "–≥–∞–¥–∞—Ç—å", "–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç—å"
        ]
        
        response_lower = ai_response.lower()
        has_unknown_phrase = any(phrase in response_lower for phrase in unknown_phrases)
           return has_unknown_phrase

    def generate_internet_answer(self, query, search_results, author_style):
        """–ì–µ–Ω–∏—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        if not search_results:
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∏–ª–µ –∞–≤—Ç–æ—Ä–∞ —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–¥–∞–Ω–Ω—ã–º–∏
        if author_style == "–ø—É—à–∫–∏–Ω":
            intro = f"–û, –º–æ–π –¥–æ—Ä–æ–≥–æ–π —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫! –ö–∞—Å–∞—Ç–µ–ª—å–Ω–æ '{query}', –ø–æ–∑–≤–æ–ª—å—Ç–µ –º–Ω–µ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —Å–≤–µ–¥–µ–Ω–∏—è–º–∏ –∏–∑ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:"
        elif author_style == "–¥–æ—Å—Ç–æ–µ–≤—Å–∫–∏–π":
            intro = f"–ú–∏–ª—ã–π –º–æ–π, –≤–∞—à –≤–æ–ø—Ä–æ—Å –æ '{query}' –∑–∞—Å—Ç–∞–≤–∏–ª –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –Ω—ã–Ω–µ—à–Ω–∏–º –∑–Ω–∞–Ω–∏—è–º —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞:"
        elif author_style == "—Ç–æ–ª—Å—Ç–æ–π":
            intro = f"–î–æ—Ä–æ–≥–æ–π —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫, –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ '{query}', —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–∞—É–∫–∞ —Å–æ–æ–±—â–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ:"
        elif author_style == "—á–µ—Ö–æ–≤":
            intro = f"–ó–Ω–∞–µ—Ç–µ, –≤–∞—à –≤–æ–ø—Ä–æ—Å –æ '{query}' –¥–æ–≤–æ–ª—å–Ω–æ –∏–Ω—Ç–µ—Ä–µ—Å–µ–Ω. –í–æ—Ç —á—Ç–æ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö:"
        else:  # –≥–æ–≥–æ–ª—å
            intro = f"–ê—Ö, –∫–∞–∫–æ–π –ª—é–±–æ–ø—ã—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ '{query}'! –ü–æ–∑–≤–æ–ª—å—Ç–µ –º–Ω–µ —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ –≥–æ–≤–æ—Ä—è—Ç –æ–± —ç—Ç–æ–º –Ω—ã–Ω–µ:"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
        main_content = []
        for i, result in enumerate(search_results[:2], 1):
            snippet = result['content']
            if len(snippet) > 150:
                snippet = snippet[:147] + "..."
            main_content.append(f"\n{snippet}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—á–∞–Ω–∏–µ –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        sources_note = "\n\n*–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∞ –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤*"
        
        return intro + "".join(main_content) + sources_note

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
internet_searcher = InternetSearcher()
