import aiohttp
import asyncio
from bs4 import BeautifulSoup
import re
from concurrent.futures import ThreadPoolExecutor
class InternetSearcher:
    def __init__(self):
        self.session = None
    
    async def search_online(self, query, max_results=3):
        """–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –ø—Ä–∏ –Ω–µ–∑–Ω–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        try:
            print(f"üîç –ë–æ—Ç –Ω–µ –∑–Ω–∞–µ—Ç –æ—Ç–≤–µ—Ç, –∏—â—É –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {query}")
            
            from googlesearch import search
            
            results = []
            # –ò—â–µ–º –≤ Google –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                try:
                    search_results = await asyncio.wait_for(
                        loop.run_in_executor(
                            executor,
                            lambda: list(search(query, num_results=max_results, lang="ru"))
                        ),
                        timeout=10.0  # –¢–∞–π–º–∞—É—Ç 10 —Å–µ–∫—É–Ω–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞
                    )
                except asyncio.TimeoutError:
                    print("‚è∞ –¢–∞–π–º–∞—É—Ç –ø–æ–∏—Å–∫–∞ –≤ Google")
                    return []

            async with aiohttp.ClientSession() as session:
                tasks = []
                for url in search_results[:max_results]:
                    tasks.append(self.fetch_page_content(session, url))
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
                try:
                    pages_content = await asyncio.wait_for(
                        asyncio.gather(*tasks, return_exceptions=True),
                        timeout=15.0  # –¢–∞–π–º–∞—É—Ç 15 —Å–µ–∫—É–Ω–¥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
                    )
                except asyncio.TimeoutError:
                    print("‚è∞ –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü")
                    return []
                
                for url, content in zip(search_results, pages_content):
                    if content and not isinstance(content, Exception):
                        results.append({
                            'url': url,
                            'content': self.clean_content(content)[:300]
                        })
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            return results
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    async def fetch_page_content(self, session, url):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            timeout = aiohttp.ClientTimeout(total=8)
            async with session.get(url, timeout=timeout) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    text = soup.get_text()
                    return text
                return None
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None
    
    def clean_content(self, text):
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç"""
        if not text:
            return ""
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def should_search_internet(self, ai_response, user_question):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–∫–∞—Ç—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
        question_lower = user_question.lower().strip()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã –∏ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è
        common_phrases = [
            "–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å", "–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä", "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ",
            "–∫–∞–∫ –¥–µ–ª–∞", "–∫–∞–∫ —Ç—ã", "–∫–∞–∫ —É —Ç–µ–±—è", "—á—Ç–æ –¥–µ–ª–∞–µ—à—å", "—á–µ–º –∑–∞–Ω–∏–º–∞–µ—à—å—Å—è",
            "—Ä–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ", "–∫—Ç–æ —Ç—ã", "—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å", "–ø–æ–º–æ–≥–∏", "—Å–ø–∞—Å–∏–±–æ", "–ø–æ–∫–∞"
        ]
        
        # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—â–∞—è —Ñ—Ä–∞–∑–∞ - –ù–ï –∏—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        if any(phrase in question_lower for phrase in common_phrases):
            return False
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ö–û–ù–ö–†–ï–¢–ù–´–• —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        factual_keywords = [
            "–∫–æ–≥–¥–∞ —Ä–æ–¥–∏–ª—Å—è", "–∫–æ–≥–¥–∞ —É–º–µ—Ä", "–∫–æ–≥–¥–∞ –Ω–∞–ø–∏—Å–∞–ª", "–≤ –∫–∞–∫–æ–º –≥–æ–¥—É",
            "–≥–¥–µ —Ä–æ–¥–∏–ª—Å—è", "–≥–¥–µ —É–º–µ—Ä", "–≥–¥–µ –Ω–∞–ø–∏—Å–∞–ª",
            "—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫—Ç–æ —Ç–∞–∫–æ–π", "–∫—Ç–æ —Ç–∞–∫–∞—è",
            "–∫–∞–∫–æ–µ –ø–µ—Ä–≤–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ", "–∫–∞–∫–æ–µ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ",
            "–ø–µ—Ä–≤–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ", "–ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ",
            "—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π", "—Å–∫–æ–ª—å–∫–æ –∫–Ω–∏–≥",
            "–¥–∞—Ç–∞ —Ä–æ–∂–¥–µ–Ω–∏—è", "–¥–∞—Ç–∞ —Å–º–µ—Ä—Ç–∏", "–±–∏–æ–≥—Ä–∞—Ñ–∏—è",
            "–∫–∞–∫–æ–π —Ä–æ–º–∞–Ω", "–∫–∞–∫–∞—è –∫–Ω–∏–≥–∞", "–∫–∞–∫–æ–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ"
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¢–û–õ–¨–ö–û –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        is_factual_question = any(keyword in question_lower for keyword in factual_keywords)
        
        # –ò—â–µ–º –¢–û–õ–¨–ö–û –µ—Å–ª–∏ —ç—Ç–æ —è–≤–Ω—ã–π —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å
        return is_factual_question

    def generate_internet_answer(self, query, search_results, author_style):
        """–ì–µ–Ω–∏—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        if not search_results:
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∏–ª–µ –∞–≤—Ç–æ—Ä–∞ —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–¥–∞–Ω–Ω—ã–º–∏
        if author_style == "–ø—É—à–∫–∏–Ω":
            intro = f"–û, –º–æ–π –¥–æ—Ä–æ–≥–æ–π —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫! –ö–∞—Å–∞—Ç–µ–ª—å–Ω–æ '{query}', –ø–æ–∑–≤–æ–ª—å—Ç–µ –º–Ω–µ –ø–æ–¥–µ–ª–∏—Ç—å—Å—è –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —Å–≤–µ–¥–µ–Ω–∏—è–º–∏ –∏–∑ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:"
        elif author_style == "–¥–æ—Å—Ç–æ–µ–≤—Å–∫–∏–π":
            intro = f"–ú–∏–ª—ã–π –º–æ–π, –≤–∞—à –≤–æ–ø—Ä–æ—Å –æ '{query}' –∑–∞—Å—Ç–∞–≤–∏–ª –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –Ω—ã–Ω–µ—à–Ω–∏–º –∑–Ω–∞–Ω–∏—è–º —á–µ–ª–æ–≤–µ—á–µ—Å—Ç–≤–∞:"
        elif author_style == "—Ç–æ–ª—Å—Ç–æ–π":
            intro = f"–î–æ—Ä–æ–≥–æ–π —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫, –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ '{query}', —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –Ω–∞—É–∫–∞ —Å–æ–æ–±—â–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ:"
        elif author_style == "—á–µ—Ö–æ–≤":
            intro = f"–ó–Ω–∞–µ—Ç–µ, –≤–∞—à –≤–æ–ø—Ä–æ—Å –æ '{query}' –¥–æ–≤–æ–ª—å–Ω–æ –∏–Ω—Ç–µ—Ä–µ—Å–µ–Ω. –í–æ—Ç —á—Ç–æ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö:"
        else:  # –≥–æ–≥–æ–ª—å
            intro = f"–ê—Ö, –∫–∞–∫–æ–π –ª—é–±–æ–ø—ã—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ '{query}'! –ü–æ–∑–≤–æ–ª—å—Ç–µ –º–Ω–µ —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ –≥–æ–≤–æ—Ä—è—Ç –æ–± —ç—Ç–æ–º –Ω—ã–Ω–µ:"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
        main_content = []
        for i, result in enumerate(search_results[:2], 1):
            snippet = result['content']
            if len(snippet) > 150:
                snippet = snippet[:147] + "..."
            main_content.append(f"\n{snippet}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—á–∞–Ω–∏–µ –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        sources_note = "\n\n*–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∞ –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤*"
        
        return intro + "".join(main_content) + sources_note

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
internet_searcher = InternetSearcher()
